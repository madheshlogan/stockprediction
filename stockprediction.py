# -*- coding: utf-8 -*-
"""gdsc.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/132Fkg_Lnu4nPTmizMnKW0yEvy4ss8w7_
"""

import torch
import pandas as pd
import numpy as np
from torch import nn

train = pd.read_csv('/content/XPREP8.csv') # reading the data

"""**data** **exploration**"""

print(train.head()) #getting data for reference

print(train.describe()) #for getting insights about the data

print(train.corr()) #corr relation matrix for getting the dependencies between columns

"""**classification model**"""

data=train[['percentage_returns given','RSI','MACD','ROC']] #taking the top 4 column for classifying the worked data
data=torch.tensor(np.array(data),dtype=torch.float32) #converting the dataframe to np array for changing to tensor

#neural network
class sm1(nn.Module):
  def __init__(self,inf):
    super().__init__()
    self.layerstack=nn.Sequential(
        nn.Linear(in_features=inf,out_features=32),
        nn.ReLU(),
        nn.Linear(in_features=32,out_features=32),
        nn.ReLU(),
        nn.Linear(in_features=32,out_features=1)
    )
  def forward(self,x):
    return self.layerstack(x)

#model initializing
model_0=sm1(4) #4 input features

# helper function
def accuracy_fn(y_true, y_pred):
    correct = torch.eq(y_true, y_pred).sum().item()
    acc = (correct / len(y_pred)) * 100
    return acc

#loss function
loss_fn=nn.BCEWithLogitsLoss()

#optimizer
optimizer=torch.optim.SGD(params=model_0.parameters(),lr=0.001) #optimizer for the model with lr 0.001 for preventing overshoot

# train test split it can also be done using the sklearn train_test_split
X_train,y_train=data[:750],torch.tensor(np.array(train['worked?'][:750]),dtype=torch.float32)
X_test,y_test=data[750:],torch.tensor(np.array(train['worked?'][750:]),dtype=torch.float32)

# Device-agnostic code  for making the runtime to cuda
model_0.to(device)
torch.manual_seed(42)
epochs = 1500

# for getting the data for plot
epochl,l1l,l2l=list(),list(),list()

# Device-agnostic code  for making the runtime to cuda
X_train, y_train = X_train.to(device), y_train.to(device)
X_test, y_test = X_test.to(device), y_test.to(device)
for epoch in range(epochs):
    model_0.train()
    y_logits = model_0(X_train).squeeze()
    y_pred = torch.round(torch.sigmoid(y_logits))
    loss = loss_fn(y_logits,y_train)
    acc = accuracy_fn(y_true=y_train,y_pred=y_pred)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    model_0.eval()
    with torch.inference_mode():
        test_logits = model_0(X_test).squeeze()
        test_pred = torch.round(torch.sigmoid(test_logits))
        test_loss = loss_fn(test_logits,y_test)
        test_acc = accuracy_fn(y_true=y_test,y_pred=test_pred)
    if epoch % 100 == 0:
        print(f"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {acc:.2f}% | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%")
    if epoch%10==0:
      epochl.append(epoch)
      l2l.append(test_loss.detach().cpu().numpy())
      l1l.append(loss.detach().cpu().numpy())

#plotting the loss curve
from matplotlib import pyplot as plt #plotting
plt.plot(epochl,l1l, label="Train loss")
plt.plot(epochl, l2l, label="Test loss")
plt.title("Training and test loss curves")
plt.ylabel("Loss")
plt.xlabel("Epochs")
plt.legend();

"""**regression model**"""

data2=train[['percentage_returns given','RSI','MACD','ROC','EMA5','EMA13','EMA26','worked?']]
# Normalize features
scaler = StandardScaler()
X_normalized = scaler.fit_transform(data2)

# Convert to PyTorch tensors
X_tensor = torch.FloatTensor(X_normalized[:750]).to(device)
y_tensor = torch.FloatTensor(y.values)[:750].reshape(-1, 1).to(device)
X_tensor_test = torch.FloatTensor(X_normalized[750:]).to(device)
y_tensor_test = torch.FloatTensor(y.values)[750:].reshape(-1, 1).to(device)
epochl,l1,l2=list(),list(),list()
# Initialize model
model = sm1(inf=8).to(device)  # 8 input features

#helper function
def mae_fn(y_true, y_pred):
    mae = torch.abs(y_true - y_pred).mean().item()
    return mae

# Loss function
loss_fn1 = nn.MSELoss()

# Optimizer
optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)

# Training loop (simplified)
for epoch in range(700):  # number of epochs
    # Forward pass
    outputs = model(X_tensor)
    loss = loss_fn1(outputs, y_tensor)

    # Backward pass and optimize
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    model_0.eval()
    with torch.inference_mode():
        test_pred1 = model(X_tensor_test).squeeze()
        test_loss1 = loss_fn1(test_pred1,y_tensor_test)
    if epoch%100==0:
      print(f"Epoch: {epoch} | Loss: {loss.item():.5f} | test loss :{test_loss1}")
      epochl.append(epoch)
      l2.append(test_loss1.detach().cpu().numpy())
      l1.append(loss.detach().cpu().numpy())

#ploting for getting inference
from matplotlib import pyplot as plt
plt.plot(epochl,l1, label="Train loss")
plt.plot(epochl, l2, label="Test loss")
plt.title("Training and test loss curves")
plt.ylabel("Loss")
plt.xlabel("Epochs")
plt.legend();

